# Deep semi-supervised learning

This repository contains deep semi-supervised learning papers mentioned in our survey.



## Content

1.   [Survey](#content)
2.   [Deep Generative methods](#content)
3.   [Consistency regularization methods](#content)
4.   [Graph-based methods](#content)
5.   [Pseudo-labeling methods](#content)
6.   [Future trends](#content)
7.   [Test results](#content)

## [Survey](#content)

- A Survey on Deep Semi-Supervised Learning. [[pdf]](https://arxiv.org/pdf/2103.00550.pdf)
  - Xiangli Yang, Zixing Song, Irwin King. *2021*

- Graph-based Semi-supervised Learning: A Comprehensive Review,[[pdf]](https://arxiv.org/abs/2102.13303) [[code]](https://github.com/AnthonySong98/awesome-graph-based-semi-supervised-learning)
  - Song, Z., Yang, X., Xu, Z., & King, I. *2021*

- A survey on semi-supervised learning.
  [[pdf]](https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf)
  - Jesper E Van Engelen, Holger H Hoos. *2020*
- An Overview of Deep Semi-Supervised Learning.
  [[pdf]](https://arxiv.org/abs/2006.05278)
  - Yassine Ouali, Céline Hudelot, Myriam Tami. *2020*
- Realistic Evaluation of Deep Semi-Supervised Learning Algorithms.
    [[pdf]](https://arxiv.org/abs/1804.09170)
    [[code]](https://github.com/brain-research/realistic-ssl-evaluation)
    - Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, Ian J. Goodfellow. *NeurIPS 2018*

## [Deep Generative methods](#content)

### Semi-supervised VAEs

- Unified Robust Semi-Supervised Variational Autoencoder.
  [[pdf]](http://proceedings.mlr.press/v139/chen21a.html)
  - Xu Chen. *ICML 2021*

- Learning Disentangled Representations with Semi-Supervised Deep Generative Models.
  [[pdf]](https://arxiv.org/pdf/1705.09783v3.pdf) 
  [[code]](https://github.com/probtorch/probtorch)
  - N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah D. Goodman, Pushmeet Kohli, Frank Wood, Philip H.S. Torr. *NeurIPS 2017*

- Infinite Variational Autoencoder for Semi-Supervised Learning.
  [[pdf]](https://arxiv.org/abs/1611.07800) 
  - Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel. *CVPR 2017*

- Auxiliary Deep Generative Models.
  [[pdf]](https://arxiv.org/abs/1602.05473) 
  - Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, Ole Winther. *ICML 2016*

- Semi-supervised Learning with Deep Generative Models.
  [[pdf]](https://arxiv.org/abs/1406.5298) 
  - Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling. *NeurIPS 2014*

### Semi-supervised GANs
- Regularizing Discriminative Capability of CGANs for Semi-Supervised Generative Learning.
  [[pdf]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Regularizing_Discriminative_Capability_of_CGANs_for_Semi-Supervised_Generative_Learning_CVPR_2020_paper.pdf) 
  - Yi Liu, Guangchang Deng, Xiangping Zeng, Si Wu, Zhiwen Yu, Hau-San Wong. *CVPR 2020*

- MarginGAN: Adversarial Training in Semi-Supervised Learning.
  [[pdf]](https://arxiv.org/abs/1704.03817)
  [[code]](https://github.com/xdu-DJhao/MarginGAN) 
  - Jinhao Dong, Tong Lin. *NeurIPS 2019*

- Enhancing TripleGAN for Semi-Supervised Conditional Instance Synthesis and Classification.
  [[pdf]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Enhancing_TripleGAN_for_Semi-Supervised_Conditional_Instance_Synthesis_and_Classification_CVPR_2019_paper.pdf) 
  - Si Wu, Guangchang Deng, Jichang Li, Rui Li, Zhiwen Yu, Hau-San Wong. *CVPR 2019*

- Triple Generative Adversarial Nets.
  [[pdf]](https://arxiv.org/abs/1703.02291) 
  - Chongxuan Li, Kun Xu, Jun Zhu, Bo Zhang. *NeurIPS 2017*

- Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference.
  [[pdf]](https://arxiv.org/abs/1705.08850) 
  - Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher. *NeurIPS 2017*

- Good Semi-supervised Learning that Requires a Bad GAN.
  [[pdf]](https://arxiv.org/pdf/1705.09783v3.pdf) 
  [[code]](https://github.com/kimiyoung/ssl_bad_gan)
  - Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Ruslan Salakhutdinov. *NeurIPS 2017*

- Improved Techniques for Training GANs.
  [[pdf]](https://arxiv.org/abs/1606.03498) 
  - Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen. *NeurIPS 2016*

- Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.
  [[pdf]](https://arxiv.org/abs/1511.06390) 
  - Jost Tobias Springenberg. *ICLR 2016*

- Semi-Supervised Learning with Generative Adversarial Networks.
  [[pdf]](https://arxiv.org/abs/1606.01583) 
  - Augustus Odena. *Preprint 2016*

## [Consistency regularization methods](#content)

- Unsupervised Data Augmentation for Consistency Training. 
  [[pdf]](https://arxiv.org/abs/1904.12848)
  [[code]](https://github.com/google-research/uda)
  - Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le. *NeurIPS 2020*

- WCP: Worst-Case Perturbations for Semi-Supervised Deep Learning. 
  [[pdf]](http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_WCP_Worst-Case_Perturbations_for_Semi-Supervised_Deep_Learning_CVPR_2020_paper.pdf)
  [[code]](https://github.com/maple-research-lab/WCP)
  - Liheng Zhang, Guo-Jun Qi. *CVPR 2020*

Dual Student: Breaking the Limits of the Teacher in Semi-Supervised Learning. 
  [[pdf]](https://arxiv.org/abs/1909.01804)
  [[code]](https://github.com/ZHKKKe/DualStudent)
  - Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, Rynson W.H. Lau. *ICCV 2019*

- There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average. 
  [[pdf]](https://openreview.net/pdf?id=rkgKBhA5Y7)
  [[code]](https://github.com/benathi/fastswa-semi-sup)
  - Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson. *ICLR 2019*

- Adversarial Dropout for Supervised and Semi-Supervised Learning.
  [[pdf]](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16322) 
  -  Sungrae Park, JunKeon Park, Su-Jin Shin, Il-Chul Moon. *AAAI 2018*

- Virtual adversarial training: a regularization method for supervised and semi-supervised learning.
  [[pdf]](https://arxiv.org/abs/1704.03976)
  [[code]](https://github.com/lyakaap/VAT-pytorch)
  - Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii. *TPAMI 2018*

- Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.
  [[pdf]](https://arxiv.org/abs/1703.01780) 
  [[code]](https://github.com/CuriousAI/mean-teacher)
  - Antti Tarvainen, Harri Valpola.  *NeurIPS 2017*

- Temporal Ensembling for Semi-Supervised Learning.
  [[pdf]](https://arxiv.org/abs/1610.02242) 
  [[code]](https://github.com/benathi/fastswa-semi-sup)
  - Samuli Laine, Timo Aila.  *ICLR 2017*

- Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning.
  [[pdf]](https://papers.NeurIPS.cc/paper/6333-regularization-with-stochastic-transformations-and-perturbations-for-deep-semi-supervised-learning.pdf) 
  - Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen. *NeurIPS 2016*


- Semi-supervised Learning with Ladder Networks.
  [[pdf]](https://arxiv.org/abs/1507.02672)
  [[code]](https://github.com/CuriousAI/ladder) 
  - Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, Tapani Raiko. *NeurIPS 2015*

## [Graph-based methods](#content)

- Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels.
  [[pdf]](https://openreview.net/forum?id=ek0RuhPoGiD)
  - Sheng Wan, Yibing Zhan, Liu Liu, Baosheng Yu, Shirui Pan, Chen Gong. *NeurIPS 2021*

- Label Propagation for Deep Semi-supervised Learning. 
  [[pdf]](https://arxiv.org/abs/1904.04717) 
  - Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum. *CVPR 2019*

- Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates.
  [[pdf]](https://arxiv.org/abs/2006.11184)
  - Jeff Calder, Brendan Cook, Matthew Thorpe, Dejan Slepcev. *ICML 2020*

- Graph Inference Learning for Semi-supervised Classification. 
  [[pdf]](https://arxiv.org/abs/2001.06137) 
  - Chunyan Xu, Zhen Cui, Xiaobin Hong, Tong Zhang, Jian Yang, Wei Liu. *ICLR 2020*


- Learning graph representations with recurrent neural network autoencoders.

    - A. Taheri, K. Gimpel, and T. Berger-Wolf. *KDD 2018* 


- Variational graph auto-encoders. [[paper]](https://arxiv.org/pdf/1611.07308.pdf%5D)
  [[code]](https://github.com/tkipf/gae)
  
  - T. N. Kipf and M. Welling. *Arxiv 2016*
  
- Structural deep network embedding. [[pdf]](https://dl.acm.org/doi/10.1145/2939672.2939753)
  [[code]](https://github.com/suanrong/SDNE)
- D. Wang, P. Cui, and W. Zhu. *KDD 2016*

## [Pseudo-labeling methods](#content)

- Big Self-Supervised Models are Strong Semi-Supervised Learners. 
  [[pdf]](https://arxiv.org/abs/2006.10029)
  [[code]](https://github.com/google-research/simclr)
  - Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton. *NeurIPS 2020*

- Meta Pseudo Labels. 
  [[pdf]](https://arxiv.org/abs/2003.10580)
  - Hieu Pham, Qizhe Xie, Zihang Dai, Quoc V. Le. *Preprint 2020*

- Self-training with Noisy Student improves ImageNet classification. 
  [[pdf]](https://arxiv.org/abs/1911.04252)
  [[code]](https://github.com/google-research/noisystudent)
  - Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le. *CVPR 2020*

- S4L: Self-Supervised Semi-Supervised Learning. 
  [[pdf]](https://arxiv.org/abs/1905.03670)
  [[code]](https://github.com/google-research/s4l)
  - Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer. *ICCV 2019*

- Deep Co-Training for Semi-Supervised Image Recognition.
  [[pdf]](https://arxiv.org/abs/1803.05984)
  [[code]](https://github.com/AlanChou/Deep-Co-Training-for-Semi-Supervised-Image-Recognition)
  - Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, Alan Yuille. *ECCV 2018*

- Tri-net for Semi-Supervised Deep Learning.
  [[pdf]](https://www.ijcai.org/Proceedings/2018/0278.pdf) 
  - Dong-Dong Chen, Wei Wang, Wei Gao,  Zhi-Hua Zhou. *IJICAI 2018*

- Pseudo-Label :  The Simple and Efficient Semi-Supervised LearningMethod for Deep Neural Networks.
  [[pdf]](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf) 
  - Dong-Hyun Lee. *ICML Workshop 2013*



## [Hybrid methods](#content)

- FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling.
  [[pdf]](https://arxiv.org/abs/2110.08263)
  [[code]](https://github.com/TorchSSL/TorchSSL)
  - Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, Takahiro Shinozaki. *NeurIPS 2021*


- FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. 
  [[pdf]](https://arxiv.org/abs/2001.07685)
  [[code]](https://github.com/google-research/fixmatch)
  - Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel. *NeurIPS 2020*

- ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring. 
  [[pdf]](https://arxiv.org/abs/1911.09785)
  [[code]](https://github.com/google-research/remixmatch)
  - David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel. *ICLR 2020*

- DivideMix: Learning with Noisy Labels as Semi-supervised Learning. 
  [[pdf]](https://openreview.net/pdf?id=HJgExaVtwr)
  [[code]](https://github.com/LiJunnan1992/DivideMix)
  - Junnan Li, Richard Socher, Steven C.H. Hoi. *ICLR 2020*

- MixMatch: A Holistic Approach to Semi-Supervised Learning. 
  [[pdf]](https://arxiv.org/abs/1905.02249)
  [[code]](https://github.com/google-research/mixmatch)
  - David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel. *NeurIPS 2019*

- Interpolation Consistency Training for Semi-Supervised Learning. 
  [[pdf]](https://arxiv.org/abs/1903.03825)
  [[code]](https://github.com/vikasverma1077/ICT)
  - Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio and David Lopez-Paz. *IJCAI 2019*

## [Future trends](#content)

- All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training.
  [[pdf]](https://arxiv.org/abs/2104.05248)
  [[code]](https://github.com/islam-nassar/semco)
  - Islam Nassar, Samitha Herath, Ehsan Abbasnejad, Wray Buntine, Gholamreza Haffari. *CVPR 2021*

- Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning.
  [[pdf]](https://arxiv.org/abs/2007.01293)
  - Zhongzheng Ren, Raymond A. Yeh, Alexander G. Schwing. *NeurIPS 2020*
- Negative sampling in semi-supervised learning. 
  [[pdf]](https://arxiv.org/abs/1911.05166) 
  - John Chen, Vatsal Shah, Anastasios Kyrillidis. *ICML 2020*

## [Test results](#content)

Table 1: Semi-supervised GANs

| **Model**            | **MNIST**       |                 | **SVHN**        | **CIFAR-10**                  |                  |
| -------------------- | --------------- | --------------- | --------------- | ----------------------------- | ---------------- |
|                      | *n* = 100       | *n* = 500       | *n* = 1000      | *n* = 1000         *n* = 4000 |                  |
| CatGAN [35]          | 1*.*39(±0*.*28) | -               | -               | -                             | 19*.*58(±0*.*58) |
| ImprovedGAN [36]     | 0*.*93(±0*.*07) | 18*.*44(±4*.*8) | 8*.*11(±1*.*3)  | 21*.*83(±2*.*01)              | 18*.*63(±2*.*32) |
| GoodBadGAN [37]      | 0*.*80(±0*.*1)  | -               | 4*.*25(±0*.*03) | -                             | 14*.*41(±0*.*30) |
| Localized GAN [38]   | -               | 5*.*48(±0*.*29) | 4*.*73(±0*.*16) | 17*.*44(±0*.*25)              | 14*.*23(±0*.*27) |
| CT-GAN [39]          | 0*.*89(±0*.*13) | -               | -               | -                             | 9*.*98(±0*.*21)  |
| ALI [40]             | -               | -               | 7*.*42(±0*.*65) | 19*.*98(±0*.*89)              | 17*.*99(±1*.*62) |
| Augmented-BiGAN [41] | -               | 4*.*87(±1*.*6)  | 4*.*39(±1*.*2)  | 19*.*52(±1*.*5)               | 16*.*20(±1*.*6)  |
| Triple GAN [42]      | 0*.*91(±0*.*58) | -               | 5*.*77(±0*.*17) | -                             | 16*.*99(±0*.*36) |
| Enhanced TGAN [43]   | 0*.*42(±0*.*03) | -               | 2*.*97(±0*.*09) | -                             | 9*.*42(±0*.*22)  |
| MarginGAN [44]       | 3*.*53(±0*.*57) | 6*.*07(±0*.*43) | -               | 10*.*39(±0*.*43)              | 6*.*44(±0*.*1)   |
| TriangleGAN [45]     | -               | -               | -               | -                             | 16*.*80(±0*.*42) |
| Structured GAN [46]  | 0*.*89(±0*.*11) | -               | 5*.*73(±0*.*12) | -                             | 17*.*26(±0*.*69) |
| *R***3**-CGAN  [47]  | -               | -               | 2*.*97(±0*.*05) | -                             | 6*.*69(±0*.*28)  |



Table 2: Semi-supervised VAEs

| **Model**             |                             | **MNIST**        |                 | **SVHN**        | **NORB**                      | **CIFAR-10**     |                 |                 |
| --------------------- | --------------------------- | ---------------- | --------------- | --------------- | ----------------------------- | ---------------- | --------------- | --------------- |
| *n* = 100             | *n* = 600        *n* = 1000 | *n* = 3000       | *n* = 1000      | *n* = 1000      | *n*  = 1000        *n* = 4000 |                  |                 |                 |
| M1+TSVM [56]          | 11*.*82(±0*.*25)            | 5*.*72(±0*.*049) | 4*.*24(±0*.*07) | 3*.*49(±0*.*04) | 54*.*33(±0*.*11)              | 18*.*79(±0*.*05) | -               | -               |
| M2 [56]               | 11*.*97(±1*.*71)            | 4*.*94(±0*.*13)  | 3*.*6(±0*.*56)  | 3*.*92(±0*.*63) | -                             | -                | -               | -               |
| M1+M2 [56]            | 3*.*33(±0*.*14)             | 2*.*59(±0*.*05)  | 2*.*40(±0*.*02) | 2*.*18(±0*.*04) | 36*.*02(±0*.*10)              | -                | -               | -               |
| ADGM [57]             | 0*.*96(±0*.*02)             | -                | -               | -               | 22*.*86                       | 10*.*06(±0*.*05) | -               | -               |
| SDGM [57]             | 1*.*32(±0*.*07)             | -                | -               | -               | 16*.*61(±0*.*24)              | 9*.*40(±0*.*04)  | -               | -               |
| Infinite VAE [58]     | 3*.*93(±0*.*5)              | -                | 2*.*29(±0*.*2)  | -               | -                             | -                | 8*.*72(±0*.*45) | 7*.*78(±0*.*13) |
| Disentangled VAE [59] | 9*.*71(±0*.*91)             | 3*.*84(±0*.*86)  | 2*.*88(±0*.*79) | 1*.*57(±0*.*93) | 38*.*91(±1*.*06)              | -                | -               | -               |
| SDVAE [60]            | 2*.*71(±0*.*32)             | 1*.*97(±0*.*14)  | 1*.*29(±0*.*11) | 1*.*00(±0*.*05) | 29*.*37(±0*.*12)              | -                | -               | -               |



Table 3: Consistency Regularization methods

| Model                             | MNIST           |                 | SVHN            |                 |                  | CIFAR-10         |                  | CIFAR-100        |                  |
| --------------------------------- | --------------- | --------------- | --------------- | --------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| *n*  = 100             *n* = 1000 | *n* = 250       | *n* = 500       | *n* = 1000      | *n* = 1000      | *n* = 2000       | *n* = 4000       | 10000            |                  |                  |
| Ladder Network [66]               | 1*.*06(±0*.*37) | 0*.*87(±0*.*08) | -               | -               | -                | -                | -                | 20*.*40(±0*.*47) | -                |
| Π Model  [67]                     | 0*.*55(±0*.*16) | -               | -               | 6*.*65(±0*.*53) | 4*.*82(±0*.*17)  | -                | -                | 12*.*36(±0*.*31) | 39*.*19(±0*.*36) |
| Temporal Ensembling [68]          | -               | -               | -               | 5*.*12(±0*.*13) | 4*.*42(±0*.*16)  | -                | -                | 12*.*16(±0*.*24) | 38*.*65(±0*.*51) |
| Mean Teacher [69]                 | -               | -               | 4*.*35(±0*.*50) | 4*.*18(±0*.*27) | 3*.*95(±0*.*19)  | 21*.*55(±1*.*48) | 15*.*73(±0*.*31) | 12*.*31(±0*.*28) | -                |
| VAT [70]                          | 1*.*36(±0*.*03) | 1*.*27(±0*.*11) | -               | -               | 6*.*83(±0*.*24)  | -                | -                | 14*.*87(±0*.*13) | -                |
| Dual Student [71]                 | -               | -               | 4*.*24(±0*.*10) | 3*.*96(±0*.*15) | -                | 15*.*74(±0*.*45) | 11*.*47(±0*.*14) | 9*.*65(±0*.*12)  | 33*.*08(±0*.*27) |
| SWA [72]                          | -               | -               | -               | -               | -                | 6*.*6            | 5*.*7            | 5*.*0            | 28*.*0           |
| VAdD [73]                         | -               | 0*.*99(±0*.*07) | -               | -               | 4*.*16(±0*.*08)  | -                | -                | 11*.*68(±0*.*19) | -                |
| WCP [74]                          | -               | -               | 4*.*29(±0*.*10) | 3*.*75(±0*.*11) | 3*.*58(±0*.*186) | 17*.*62(±1*.*52) | 11*.*93(±0*.*39) | 9*.*72(±0*.*31)  | -                |
| UDA [75]                          | -               | -               | -               | -               | 2*.*55(±0*.*99)  | -                | -                | 5*.*29(±0*.*25)  | -                |

Table 4: Pseudo-labeling methods

| Model                  | MNIST           | SVHN            | CIFAR-10        | CIFAR-100        | STL10           | ImageNet |         |         |
| ---------------------- | --------------- | --------------- | --------------- | ---------------- | --------------- | -------- | ------- | ------- |
| *n* = 100              | *n* = 1000      | *n* = 4000      | *n* = 10000     | 1000    5000     | Top-1     Top-5 |          |         |         |
| Deep Co-training [104] | -               | 3*.*61(±0*.*15) | 9*.*03(±0*.*18) | 38*.*77(±0*.*28) | -               | -        | 46*.*50 | 22*.*73 |
| Tri-Net [105]          | 0*.*53(±0*.*10) | 3*.*71(±0*.*14) | 8*.*45(±0*.*22) | -                | -               | -        | -       | -       |
| *S*4L-MOAM  [106]      | -               | -               | -               | -                | -               | -        | 26*.*79 | 8*.*77  |
| MPL [107]              | -               | 1*.*99(±0*.*07) | 3*.*89(±0*.*07) | -                | -               | -        | 26*.*11 | -       |
| EnAET [108]            | -               | 2*.*42          | 4*.*18          | 22*.*92          | 8*.*04          | 4*.*52   | -       | -       |
| SimCLRv2 [109]         | -               | -               | -               | -                | -               | -        | 19*.*1  | 4*.*5   |

Table 5: Hybrid methods

| **Model**  |                             | **SVHN**        |                 |                                               | **CIFAR-10**     |                               | **CIFAR-100**    | **STL10**       |                  |                  |                  |                  |
| ---------- | --------------------------- | --------------- | --------------- | --------------------------------------------- | ---------------- | ----------------------------- | ---------------- | --------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| *n* = 250  | *n* = 500        *n* = 1000 | *n* = 4000      | *n* = 40        | *n* = 250         *n* = 1000             4000 | *n* = 400        | *n* = 2500              10000 | 1000             |                 |                  |                  |                  |                  |
| ICT        | 4*.*78(±0*.*68)             | 4*.*23(±0*.*15) | 3*.*89(±0*.*04) | -                                             | -                | -                             | 15*.*48(±0*.*78) | 7*.*29(±0*.*02) | -                | -                | -                | -                |
| MixMatch   | 3*.*78(±0*.*26)             | 3*.*64(±0*.*46) | 3*.*27(±0*.*31) | 2*.*89(±0*.*06)                               | -                | -                             | -                | 4*.*95(±0*.*08) | -                | -                | 25*.*88(±0*.*30) | 10*.*18(±1*.*46) |
| ReMixMatch | 3*.*10(±0*.*50)             | -               | 2*.*83(±0*.*30) | 2*.*42(±0*.*09)                               | 19*.*10(±9*.*64) | 6*.*27(±0*.*34)               | 5*.*73(±0*.*16)  | 5*.*14(±0*.*04) | 44*.*28(±2*.*06) | 27*.*43(±0*.*31) | 23*.*03(±0*.*56) | 6*.*18(±1*.*24)  |
| FixMatch   | 2*.*48(±0*.*38)             | -               | 2*.*28(±0*.*11) | -                                             | 11*.*39(±3*.*35) | 5*.*07(±0*.*33)               | -                | 4*.*26(±0*.*05) | 48*.*85(±3*.*01) | 28*.*29(±0*.*11) | 22*.*60(±0*.*12) | 5*.*17(±0*.*63)  |
| FlexMatch  | -                           | -               | 2*.*86(±0*.*91) | -                                             | 4*.*99(±0*.*16)  | 4*.*80(±0*.*06)               | -                | 3*.*95(±0*.*03) | 32*.*44(±1*.*99) | 23*.*85(±0*.*23) | 19*.*92(±0*.*06) | 5*.*56(±0*.*22)  |

Comparison of test error rates for different models using Wide ResNet-28-2(WRN 28-2) for CIFAR-10 and SVHN, Wide ResNet-28-8(WRN28-8) for CIFAR-100, Wide ResNet37-2 (WRN37-2) for STL-10 and ResNet50 for 10% labels of ImageNet. The value *n* stands for the number of labeled samples in the training set. The main results are reported in "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. NeurIPS 2021."# denotes that the results are reported in "Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning. AAAI 2021". * denotes that the results are reported in original paper. 



| Model                         | n=40       | n=250      | CIFAR-10(WRN28-2)      n=1000        n=2000 | n=4000        |                |
| ----------------------------- | ---------- | ---------- | ------------------------------------------- | ------------- | -------------- |
| Ladder Network                | 74.97±1.80 | 48.20±0.68 | -                                           | -             | 13.21±0.40     |
| # Ladder Network(CNN13)       | -          | -          | -                                           | -             | 12.36 ± 0.31   |
| *Pi Model(CNN13)              | -          | -          | -                                           | -             | *11.29 ±  0.24 |
| # Temporal  Ensembling(WRN28) | -          | -          | -                                           | -             | 16.37 ± 0.63   |
| *Temporal  Ensembling(CNN13)  | -          | -          | -                                           | -             | 12.16 ± 0.24   |
| Mean Teacher                  | 70.09±4.28 | 35.88±5.00 | -                                           | -             | 8.25±0.10      |
| # Mean Teacher(CNN-13)        | -          | -          | -                                           | -             | 12.31 ± 0.24   |
| VAT                           | 76.40±2.47 | 44.58±1.55 | -                                           | -             | 11.31±0.35     |
| # VAT(CNN13)                  | -          | -          | -                                           | -             | 11.36 ± 0.34   |
| # VAT + EntMin(WRN28)         | -          | -          | -                                           | -             | 13.13 ± 0.39   |
| # VAT+EntMIn(CNN13)           | -          | -          | -                                           | -             | 10.55 ± 0.05   |
| *VAdD(KL)                     | -          | -          | -                                           | -             | 11.68 ± 0.19   |
| *VAdD(QE)                     | -          | -          | -                                           | -             | 11.32 ± 0.11   |
| *WCP(CNN13)                   | -          | -          | 17.62±1.52                                  | 11.93±0.39    | 9.72±0.31      |
| *Dual Student(CNN13)          | -          | -          | 15.74 ±  0.45                               | 11.47 ±  0.14 | 9.65 ± 0.12    |
| UDA                           | 7.33±2.03  | 5.11±0.07  | -                                           | -             | 4.18±0.10      |
| # TSSDL (CNN-13)              | -          | -          | -                                           | -             | 9.30 ± 0.55    |
| # DeepLP(CNN-13)              | -          | -          | -                                           | -             | 10.61 ± 0.28   |
| *DeepLP(CNN13)                | -          | -          | 22.02 ±  0.88                               | 15.66 ±  0.35 | 12.69 ± 0.29   |
| *DeepLP+MT(CNN13)             | -          | -          | 16.93 ±  0.70                               | 13.22 ±  0.29 | 10.61 ± 0.28   |
| # SNTG(CNN13)                 | -          | -          | -                                           | -             | 10.93 ± 0.14   |
| Pseudo-label                  | 69.51±4.55 | 41.02±3.56 | -                                           | -             | 13.15±1.84     |
| *Deep Co-training             | -          | -          | -                                           | -             | 9.03 ± 0.18    |
| *Tri-Net                      | -          | -          | -                                           | -             | 8.45(±0.22)    |
| *MPL+RandAugment              | -          | -          | -                                           | -             | 12.45 ± 0.14   |
| *MPL+UDA                      | -          | -          | -                                           | -             | 3.89 ± 0.07    |
| Noisy Student                 | -          | -          | -                                           | -             | 4.18           |
| # ICT(WRN28)                  | -          | -          | -                                           | -             | 7.66 ± 0.17    |
| *ICT(CNN13)                   | -          | -          | 15.48 ±  0.78                               | 9.26 ± 0.09   | 7.29 ± 0.02    |
| MixMatch                      | 36.33±8.88 | 14.51±2.31 | -                                           | -             | 6.81±0.08      |
| ReMixMatch                    | 24.38±9.32 | 9.92±4.74  | -                                           | -             | 4.98±0.11      |
| FixMatch                      | 6.78±0.50  | 4.95±0.07  | -                                           | -             | 4.09±0.02      |
| FlexMatch                     | 4.99±0.16  | 4.80±0.06  | -                                           | -             | 3.95±0.03      |



| Model                         | CIFAR-100(WRN28-8) |            |              |
| ----------------------------- | ------------------ | ---------- | ------------ |
|                               | n=400              | n=2500     | n=10000      |
| Ladder Network                | 85.90±1.42         | 58.72±0.16 | 36.36±0.26   |
| *Pi Model(CNN13)              | -                  | -          | 39.19 ± 0.36 |
| *Temporal  Ensembling (CNN13) | -                  | -          | 38.65 ± 0.51 |
| Mean Teacher                  | 78.66±1.80         | 45.48±1.90 | 31.44±0.02   |
| VAT                           | 82.12±0.02         | 48.08±0.48 | 32.30±0.13   |
| *Dual  Student(CNN13)         | -                  | -          | 33.08 ± 0.27 |
| UDA                           | 44.99±2.28         | 27.59±0.24 | 22.09±0.19   |
| *DeepLP+MT(CNN13)             | -                  | -          | 35.92 ± 0.47 |
| Pseudo-label                  | 86.10±1.50         | 58.00±0.38 | 36.48±0.13   |
| *Deep Co-training             | -                  | -          | 38.77(±0.28) |
| Noisy Student                 | -                  | -          | 22.92        |
| MixMatch                      | 64.80±1.90         | 40.05±0.33 | 27.14±0.16   |
| ReMixMatch                    | 43.39±2.44         | 26.54±0.38 | 20.28±0.20   |
| FixMatch                      | 46.76±0.79         | 28.15±0.81 | 22.47±0.66   |
| FlexMatch                     | 32.44±1.99         | 23.85±0.23 | 19.92±0.06   |



|      | Model                         |            | SVHN(WRN28-2) |             |             |      |
| ---- | ----------------------------- | ---------- | ------------- | ----------- | ----------- | ---- |
|      |                               | n=40       | n=250         | n=500       | n=1000      |      |
|      | Ladder Network                | 69.81±1.97 | 11.76±0.62    | -           | 7.04±0.24   |      |
|      | *Pi Model(CNN13)              | -          | -             | 6.65 ± 0.53 | 4.82 ± 0.17 |      |
|      | # Temporal  Ensembling(WRN28) | -          | -             | -           | 7.19 ± 0.27 |      |
|      | *Temporal  Ensembling(CNN-13) | -          | -             | 5.12 ± 0.13 | 4.42 ± 0.16 |      |
|      | Mean Teacher                  | 43.04±2.00 | 3.44±0.04     | -           | 3.27±0.00   |      |
|      | # Mean Teacher(CNN-13)        | -          | -             | -           | 3.95 ± 0.19 |      |
|      | VAT                           | 52.20±25.5 | 4.89±0.03     | -           | 4.68±0.02   |      |
|      | # VAT(CNN-13)                 | -          | -             | -           | 5.42        |      |
|      | # VAT+EntMin(WRN28)           | -          | -             | -           | 5.35 ± 0.19 |      |
|      | # VAT+EntMIn(CNN13)           | -          | -             | -           | 3.86        |      |
|      | *VAdD(KL)                     | -          | -             | -           | 4.16 ± 0.08 |      |
|      | *VAdD(QE)                     | -          | -             | -           | 4.26 ± 0.14 |      |
|      | *WCP(CNN13)                   | -          | 4.29±0.10     | 3.75±0.11   | 3.58±0.186  |      |
|      | *Dual Student(CNN13)          | -          | 4.24 ± 0.10   | 3.96 ± 0.15 | -           |      |
|      | UDA                           | 4.40±2.31  | 1.88±0.01     | -           | 1.93±0.01   |      |
|      | # TSSDL(CNN-13)               | -          | -             | -           | 3.35 ± 0.27 |      |
|      | # SNTG(CNN13)                 | -          | -             | -           | 3.86 ± 0.27 |      |
|      | Pseudo-label                  | 60.32±2.46 | 14.66±0.30    | -           | 9.56±0.25   |      |
|      | *Deep Co-training             | -          | -             | -           | 3.61 ± 0.15 |      |
|      | *Tri-Net                      | -          | -             | -           | 3.71(±0.14) |      |
|      | *MPL+RandAugment              | -          | -             | -           | 5.98 ± 0.05 |      |
|      | *MPL+UDA                      | -          | -             | -           | 1.99 ± 0.07 |      |
|      | *EnAET                        | -          | -             | -           | 2.42        |      |
|      | # ICT(WRN28)                  | -          | -             | -           | 3.53 ± 0.07 |      |
|      | *ICT(CNN13)                   | -          | 4.78 ± 0.68   | 4.23 ± 0.15 | 3.89 ± 0.04 |      |
|      | MixMatch                      | 29.58±3.92 | 4.47±0.19     | -           | 4.08±0.12   |      |
|      | ReMixMatch                    | 45.12±0.27 | 6.10±0.38     | -           | 5.86±0.18   |      |
|      | FixMatch                      | 4.36±2.16  | 2.00±0.04     | -           | 1.97±0.03   |      |
|      | FlexMatch                     | 5.36±2.38  | 3.99±1.78     | -           | 2.86±0.91   |      |
|      |                               |            |               |             |             |      |

| Model          |            | SLT-10     |            |
| -------------- | ---------- | ---------- | ---------- |
|                | n=40       | n=250      | n=1000     |
| Ladder Network | 73.31±0.02 | 54.92±3.83 | 32.22±0.66 |
| Mean Teacher   | 70.62±3.04 | 56.54±3.94 | 32.12±0.12 |
| VAT            | 74.35±0.05 | 56.46±2.39 | 36.48±1.74 |
| UDA            | 37.31±3.03 | 12.07±1.50 | 6.65±0.25  |
| Pseudo-label   | 74.48±1.48 | 55.63±5.38 | 31.80±0.29 |
| *EnAET         | -          | -          | 8.04       |
| MixMatch       | 59.46±3.49 | 34.70±0.58 | 20.23±0.36 |
| ReMixMatch     | 36.36±3.74 | 13.36±0.26 | 8.40±1.36  |
| FixMatch       | 35.42±6.43 | 10.49±1.03 | 6.20±0.20  |
| FlexMatch      | 10.87±1.15 | 7.71±0.14  | 5.56±0.20  |



| Model                    | 10% of ImageNet(ResNet50) |              |
| ------------------------ | ------------------------- | ------------ |
|                          | Top-1                     | Top-5        |
| Mean Teacher             | 37.83±0.12                | 16.65 ± 0.08 |
| # VAT                    | -                         | 17.22        |
| # VAT+EntMin             | -                         | 16.61        |
| *Dual Student(ResNeXt50) | 36.48 ± 0.05              | 16.42 ± 0.07 |
| UDA                      | 31.2                      | 11.5         |
| Pseudo-label             | -                         | 17.59        |
| *Deep Co-training        | 46.5                      | 22.73        |
| # *S*4L-Rotation         | -                         | 16.18        |
| # *S*4L-Exemplar         | -                         | 16.28        |
| *S*4L(Rot+VAT+EntMin)    | 26.8                      | 8.8          |
| *MPL+RandAugment         | 26.2                      | -            |
| SimCLRv2                 | 22.5                      | 6.6          |
| *FixMatch                | 28.54±0.52                | 10.87±0.28   |
